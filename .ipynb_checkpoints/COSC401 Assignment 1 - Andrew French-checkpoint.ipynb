{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# Andrew French - 11147452\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(19680801)\n",
    "\n",
    "def part1():\n",
    "\n",
    "    datasets = {\"iris\": load_iris(), \"breast cancer\":load_breast_cancer(), \"digits\": load_digits()}\n",
    "\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        dataset = datasets[key]\n",
    "\n",
    "        print(\"Training the\", key, \"dataset...\")\n",
    "\n",
    "        # Split the data in training and testing sets\n",
    "        print(\"Test = 40% - Train = 60%\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.4, random_state=0)\n",
    "\n",
    "        #Loop with different depths - 1 to 10\n",
    "        dataframe = pd.DataFrame(columns=['Depth', 'Avg # of Nodes', 'Accuracy'])\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        for depth in range(1, 16):\n",
    "\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "            clf = clf.fit(X_train, y_train)\n",
    "            scores = cross_val_score(clf, dataset.data, dataset.target, cv=8)\n",
    "            #Unsure if the avg # of nodes is correct\n",
    "            dataframe.loc[depth] = [clf.get_depth(), clf.tree_.node_count/8, \"%0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)]\n",
    "\n",
    "        print(dataframe)\n",
    "\n",
    "        # Select the best value of hyper-parameter and then use the entire training data to build a classifier. \n",
    "        best_value = clf.get_depth()\n",
    "        print(\"The best value for the depth hyper-parameter is\", best_value)\n",
    "\n",
    "        # Report the number of nodes and the accuracy of the classifier on the test set.\n",
    "        print(\"Testing the\", key, \"dataset with test data and with hyper-parameter...\")\n",
    "        clf = tree.DecisionTreeClassifier(max_depth=best_value)\n",
    "        clf = clf.fit(X_test, y_test)\n",
    "        scores = cross_val_score(clf, dataset.data, dataset.target, cv=8)\n",
    "        # Again not sure if number of nodes is correct\n",
    "        print(\"Number of nodes:\", clf.tree_.node_count/8)\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "        print(\"---------------------------------------------------\")\n",
    "        \n",
    "part1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def part2():\n",
    "\n",
    "    X = np.random.randint(low=-100, high=100, size=(800, 2))\n",
    "    Y = np.random.randint(low=0, high=2, size=800)\n",
    "    \n",
    "    logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "    # Create an instance of Logistic Regression Classifier and fit the data.\n",
    "    print(\"Fitting the classifer...\")\n",
    "    logreg.fit(X, Y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    print(\"Plotting the decision boundary...\")\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    print(\"Putting the result into a color plot...\")\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot also the training points\n",
    "    print(\"Plotting the training points...\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    print(\"Showing the plot:\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# Current Running Part\n",
    "part2()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def part3():\n",
    "    # Reading the file, latin encoding is used to avoid some errors\n",
    "    data = pd.read_csv('assignment1-2019-data.csv', encoding=('latin'), quoting=3)\n",
    "\n",
    "    # Creating the list of input and output variables.\n",
    "\n",
    "    inputVariables=list(data)\n",
    "    del inputVariables[4]\n",
    "    outputVariables=list(data)[4]\n",
    "    inputData=data[inputVariables]\n",
    "    \n",
    "    # Dummy variable work\n",
    "    for column in inputData.columns:\n",
    "        if inputData[column].dtype==object:\n",
    "            dummyCols=pd.get_dummies(inputData[column])\n",
    "            inputData=inputData.join(dummyCols)\n",
    "            del inputData[column]\n",
    "    \n",
    "    \n",
    "    # Iterate through A, B, C -> and create three different models for each set\n",
    "    for i in range(1, 4):\n",
    "        variables = list(inputData)\n",
    "        new_variables = variables[:3] + list(variables[2 + i])\n",
    "        updatedInputData = inputData[new_variables]\n",
    "        \n",
    "        # Fit to the model    \n",
    "\n",
    "        model_1=LinearRegression()\n",
    "        model_1.fit(updatedInputData, data[outputVariables])\n",
    "        print(\"Mean squared error:\", np.mean((model_1.predict(updatedInputData) - data[outputVariables]) ** 2))\n",
    "        print(\"RÂ²:\", model_1.score(updatedInputData, data[outputVariables]))\n",
    "\n",
    "        # Round the coefficients of the learned model to integers and \n",
    "        # write a casebased definition of the functions identified by the regression model(s).\n",
    "        coefficients = model_1.coef_\n",
    "        rounded_coefficients = list(map(lambda x: round(x), coefficients))\n",
    "        print(\"Rounded coefficients:\", rounded_coefficients)\n",
    "        \n",
    "part3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 -> Part 3\n",
    "## Depending on the method you used for regression, describe how a new\n",
    "##    (unseen) data point (an input vector of size 4) can be assigned a predicted\n",
    "##     value.\n",
    "\n",
    "If you......\n",
    "Does this mean the input is like this -> (x1, x2, x, x4)\n",
    "Or, is it like a new x5 that has (A, B, C, D) categories??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
